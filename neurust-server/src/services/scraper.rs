use crate::services::knowledge_store::KnowledgeStore;
use reqwest::Client;
use sqlx::PgPool;
use std::time::Duration;

pub struct ScraperService {
    client: Client,
    store: KnowledgeStore,
}

impl ScraperService {
    // Constructor
    pub fn new(pool: PgPool) -> Self {
        Self {
            client: Client::builder()
                .timeout(Duration::from_secs(10)) // 10s Timeout
                .user_agent("Neurust-Agent/1.0") // Fake User Agent
                .build()
                .unwrap(),
            store: KnowledgeStore::new(pool),
        }
    }

    // URL á€€á€­á€¯á€–á€á€ºá€•á€¼á€®á€¸ Clean Text á€•á€¼á€±á€¬á€„á€ºá€¸á€•á€±á€¸á€™á€Šá€·á€º Function
    pub async fn scrape_url(&self, url: &str) -> Result<String, String> {
        println!("ğŸ•·ï¸ Scraping URL: {}", url);

        let response = self
            .client
            .get(url)
            .send()
            .await
            .map_err(|e| format!("Failed to connect: {}", e))?;

        if !response.status().is_success() {
            return Err(format!("Error: HTTP {}", response.status()));
        }

        let html_content = response
            .text()
            .await
            .map_err(|e| format!("Failed to read text: {}", e))?;

        // ğŸ”¥ HTML to Clean Text (Using html2text crate)
        // width 80 characters á€”á€²á€· á€…á€¬á€…á€®á€•á€±á€¸á€™á€šá€º
        let clean_text = html2text::from_read(html_content.as_bytes(), 80);

        // á€…á€¬á€¡á€›á€™á€ºá€¸á€›á€¾á€Šá€ºá€›á€„á€º AI Token á€•á€¼á€Šá€·á€ºá€á€½á€¬á€¸á€”á€­á€¯á€„á€ºá€œá€­á€¯á€· á€¡á€œá€¯á€¶á€¸á€›á€± áˆá€á€á€ á€œá€±á€¬á€€á€ºá€•á€² á€šá€°á€™á€šá€º
        let truncated_text: String = clean_text.chars().take(8000).collect();

        Ok(truncated_text)
    }

    // URL á€€á€­á€¯á€–á€á€ºá€™á€šá€ºáŠ á€•á€¼á€®á€¸á€›á€„á€º Database á€‘á€²á€á€­á€™á€ºá€¸á€™á€šá€º
    pub async fn scrape_and_save(&self, url: &str, topic: &str) -> Result<String, String> {
        // á. scrape_url logic á€€á€­á€¯ á€œá€¾á€™á€ºá€¸á€á€±á€«á€ºá€á€¯á€¶á€¸á€™á€šá€º
        let content = self.scrape_url(url).await?;

        // á‚. á€›á€œá€¬á€á€²á€· Content á€€á€­á€¯ Database á€‘á€² Upsert á€œá€¯á€•á€ºá€™á€šá€º
        // (KnowledgeStore.save_doc á€€ DB á€¡á€á€…á€ºá€”á€²á€· á€á€»á€­á€á€ºá€•á€¼á€®á€¸á€á€¬á€¸á€•á€«)
        if let Err(e) = self.store.save_doc(url, topic, &content).await {
            eprintln!("âŒ Failed to save to DB: {}", e);
            return Err(format!("Database Error: {}", e));
        }

        println!("ğŸ’¾ Knowledge stored for topic: {}", topic);
        Ok(content)
    }
}